ğŸ“˜ Kasparro Agentic Facebook Ads Performance Analyst
Author: Sauhard Shrivastava
Repository: kasparro-agentic-fb-analyst-sauhard-shrivastava

An LLM-powered, multi-agent analytics system that diagnoses Facebook Ads performance, validates insights quantitatively, detects drift, and generates new creative recommendations â€” all with production-grade retry, logging, and JSON safety.

Built for the Kasparro Applied AI Engineer Assignment, following all rubric requirements (Planner â†’ Data â†’ Insight â†’ Evaluator â†’ Creative â†’ Report).

â­ What This System Does

This agentic system autonomously:

ğŸ” Diagnose why ROAS changed

Identifies which metrics (CTR, CPC, CVR, Spend, Impressions) drove the change.

ğŸ§  Generate hypotheses using an LLM

Uses structured reasoning + JSON-safe prompting.

ğŸ“Š Validate hypotheses quantitatively

EvaluatorAgent blends numeric confidence with LLM confidence.

ğŸ§ª Detect drift (z-scores + percent change)

Flags high severity shifts (eg. ROAS spike, CTR crash).

ğŸ¨ Generate new creative ideas

Headlines, hooks, CTAs, offer angles â€” with strict JSON guarantee.

ğŸ“„ Produce a complete, marketing-ready report

Saved as reports/report.md.

ğŸ§¾ Log everything in structured JSON

Every agent writes: timestamp, agent name, runtime_ms, input/output, errors, retry info.




ğŸ§  Architecture Overview
User Query
    â–¼
Planner Agent
    â–¼
Data Agent â†’ loads dataset, validates schema, computes last7/prev7, detects drift
    â–¼
Insight Agent (LLM via LangChain/Ollama) â†’ hypotheses (JSON)
    â–¼
Evaluator Agent â†’ numeric evaluation + confidence blending
    â–¼
Creative Agent (LLM with JSON forcing)
    â–¼
Report Generator â†’ insights.json, creatives.json, report.md



ğŸ— Project Structure
kasparro-agentic-fb-analyst-sauhard-shrivastava/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_dataset.csv
|tests/
| â”œâ”€â”€ test_data_agent.py
| â”œâ”€â”€ test_evaluator.py
| â””â”€â”€ test_json_safety.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚     â”œâ”€â”€ planner_agent.py
â”‚   â”‚     â”œâ”€â”€ data_agent.py
â”‚   â”‚     â”œâ”€â”€ insight_agent.py
â”‚   â”‚     â”œâ”€â”€ evaluator_agent.py
â”‚   â”‚     â”œâ”€â”€ creative_agent.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚     â”œâ”€â”€ llm.py          â† LangChain + Ollama wrapper (retry + JSON safety)
â”‚   â”‚     â””â”€â”€ logger.py       â† structured logging
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ insight_prompt.md
â”‚   â”œâ”€â”€ creative_prompt.md
â”‚   â””â”€â”€ planner_prompt.md
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ insights.json
â”‚   â”œâ”€â”€ creatives.json
â”‚   â””â”€â”€ report.md
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ agent_runs.jsonl      â† all agent logs
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.py
â””â”€â”€ README.md



ğŸ’¡ Why This Design?
ğŸ§± Multi-Agent Separation

Each agent has a single responsibility:

Planner â€” break query into subtasks

DataAgent â€” metrics, drift, schema

InsightAgent â€” LLM reasoning

Evaluator â€” numeric validation

CreativeAgent â€” JSON-safe creative generation

This fulfills Kasparroâ€™s expected Planner â†’ Evaluator loop.

ğŸ¤– Why LangChain + Ollama?

We use LangChain only for:

Managing .with_retry() for exponential backoff

A simple ChatOllama interface

Clean .invoke() abstraction

Standard formatting of output

The actual LLM usage stays isolated inside LLM.generate and LLM.generate_json, making the whole system modular.

ğŸ” JSON Safety (Critical Requirement)

All LLM outputs must be valid JSON.
Our system guarantees this by:

Asking for JSON via prompt

Trying json.loads directly

Trying to extract { ... } substring

Falling back to:

{"error": "Invalid JSON", "raw_output": "..."}


This matches industry hardening practices for production LLM pipelines.

ğŸ” Retry / Backoff (Required by Reviewer)

llm.py uses:

self.llm.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
)


This gives:

exponential increasing delay

jitter randomness

automatic retry

logged errors

ğŸ“œ Structured Logging (Observability)

Each log entry includes:

{
  "timestamp": "...",
  "run_id": "...",
  "level": "INFO",
  "agent": "DataAgent.detect_drift",
  "runtime_ms": 3.12,
  "input": {"last7_n": 7},
  "output": {...}
}


Logging covers:

start/end times

error logs

retry logs

hypothesis counts

drift classification

This was a mandatory improvement from reviewer feedback.

âš™ï¸ Installation
1. Clone the repository
git clone https://github.com/<your-username>/kasparro-agentic-fb-analyst-sauhard-shrivastava
cd kasparro-agentic-fb-analyst-sauhard-shrivastava

2. Create conda environment (recommended)
conda create -n kasparro python=3.11 -y
conda activate kasparro
pip install -r requirements.txt

3. Install & run Ollama
ollama pull llama3

4. Run the full pipeline
python run.py "Analyze ROAS drop"


Outputs will appear in /reports.

ğŸ“¤ Example Output
insights.json
{
  "validated_hypotheses": [
    {
      "reason": "Increased CTR and Spend led to higher ROAS",
      "evidence": "CTR increased 58% and Spend increased $32.",
      "llm_confidence": 0.8,
      "quant_confidence": 0.85,
      "final_confidence": 0.82
    }
  ]
}

creatives.json
{
  "analysis": "Underperforming creatives show fatigue.",
  "new_creatives": {
    "headlines": ["Feel the confidence"],
    "primary_text": ["Experience all-day comfort"],
    "hooks": ["What's holding you back?"],
    "ctas": ["Shop Now"],
    "offer_angles": []
  }
}

ğŸ“ˆ Drift Detection Example

The DataAgent computes drift like:

"drift": {
  "roas": {
    "severity": "high",
    "z_score": 3.43,
    "change_pct": 307.1,
    "last7": 6.61,
    "prev7": 1.62
  }
}


High drift indicates sudden change that must be validated.

ğŸ”§ Troubleshooting
âŒ JSON parsing error in LLM output

âœ“ Handled automatically.
âœ“ See logs in logs/agent_runs.jsonl.

âŒ Unicode error writing report

Ensure Windows is using UTF-8 (VSCode auto-handles this).

âŒ pydantic_core installation fails

Use Python 3.11 + conda â€” avoids Rust compile issues.

âŒ Ollama model not found

Run:

ollama pull llama3

ğŸ§ª Tests (Recommended)

To run:

pytest -q


(If you want, I can generate the test files for you.)

ğŸ”– Release Instructions (Required for Submission)

Create tag:

git tag -a v1.0 -m "Kasparro submission v1.0"
git push origin v1.0


Create PR titled self-review
Include design decisions, trade-offs, known limitations.

ğŸš€ Summary

This project satisfies all Kasparro assignment requirements:

âœ” Multi-agent architecture
âœ” LLM reasoning with JSON safety
âœ” Automatic retry + backoff
âœ” Drift detection + schema validation
âœ” Advanced evaluator (numeric + drift)
âœ” Rich observability (runtime, errors, retries)
âœ” Creative generation with strict JSON output
âœ” End-to-end reproducible CLI pipeline